{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7487ec14-0b60-4626-8b77-09c47c8dd019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbee4b9-ea06-470a-9d85-49f0ecba243a",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "\n",
    "In modeling we make the assumption that the data follows some unknown function with some noise:\n",
    "\n",
    "$$\n",
    "f(X) + \\epsilon\n",
    "$$\n",
    "\n",
    "Then we create a model $\\hat{f}(X)$ that approximates this model as best it can!\n",
    "- Note that $\\epsilon$ is fixed. It represents a barrier to our predictive capabilities in some sense.\n",
    "\n",
    "The **Bias** of our model $\\hat{f}(X)$ is the expected error of our model:\n",
    "$$\n",
    "\\text{E}[(f(X) - \\hat{f}(X)]\n",
    "$$\n",
    "However, we don't know $f(X)$ in practice so we often use the loss of our model against the training set as a stand-in for bias. In other words the bias refers to how well the model is at predicting the target variable.\n",
    "\n",
    "The **Variance** of our model $\\text{Var}[\\hat{f}(X)]$ represents how sensitive our model is to perturbations in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea98a98a-0d57-46f1-9a41-60cccdce7db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use similar fake nonlinear data as when we discussed under/over fitting\n",
    "num_pts = 100\n",
    "X = np.linspace(-2, 2, num_pts)\n",
    "\n",
    "# noise\n",
    "epsilon = np.random.normal(0, 3, num_pts)\n",
    "\n",
    "# degree 3 polynomial\n",
    "Y = 3*(X-1)*(X+2)*(X-1.5) + epsilon\n",
    "\n",
    "plt.scatter(x=X, y=Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114713db-3a86-448a-848b-dc1a5dd38731",
   "metadata": {},
   "source": [
    "- Since the data is non-linear, a linear model has high *Bias* here. We see that the expected error is quite large.\n",
    "- The linear model also has low *Variance*. If I perturb the input variable $X$, the output is perturbed in a linear fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d9cfae-cfa8-4763-a7e7-ab73767635d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(Y, sm.add_constant(X), hasconst=True)\n",
    "res = model.fit()\n",
    "b, m = res.params\n",
    "print(b, m)\n",
    "\n",
    "# plot the points\n",
    "plt.scatter(x=X, y=Y)\n",
    "\n",
    "# plot the line\n",
    "plt.axline((0, b), slope=m, color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd5ee5e-7b2f-4c51-a517-0e226ea6ef16",
   "metadata": {},
   "source": [
    "Let's fit a high degree polynomial to the data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71404cc-267d-49d6-918b-392527e8ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x':X, 'y':Y})\n",
    "\n",
    "n = 30\n",
    "\n",
    "for i in range(n-1):\n",
    "    df[f'x_{i+2}'] = X**(i+2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada92a9d-814b-48cd-b80e-eed635280cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_var = 'x'\n",
    "for i in range(n-1):\n",
    "    indep_var = indep_var + f' + x_{i+2}'\n",
    "print(indep_var)\n",
    "\n",
    "model = ols(formula = f'y ~ {indep_var}', data=df)\n",
    "res = model.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbd0df7-d46b-463c-bb2e-673ef301300f",
   "metadata": {},
   "source": [
    "- Note the \"squiggliness\" of the model. This model might have decent Bias, but it has high variance as well.\n",
    "- If we perturb the input variable left or right we might drastically change the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d629aa4-68b0-4095-b35c-c83c97f06fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=X, y=Y)\n",
    "\n",
    "# plot the polynomial\n",
    "coefs = list(res.params)\n",
    "coefs.reverse()\n",
    "poly = [np.polyval(coefs, i) for i in X]\n",
    "plt.plot(X, poly)\n",
    "\n",
    "plt.xlim(min(X), max(X))\n",
    "plt.ylim(min(Y), max(Y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73540ed1-3c61-4e07-afe6-480d69f5cf69",
   "metadata": {},
   "source": [
    "The **Bias-Variance Tradeoff** is the trade-off between reducing Bias and reducing Variance.\n",
    "- Often to reduce Bias you have to fit the model more closely to the training data. This might result in a high Variance.\n",
    "- Often to reduce Variance you need to make the model less sensitive to perturbations. This might result in a higher Bias since you are fitting the training data less closely.\n",
    "\n",
    "This tradeoff relates to the underfitting vs. overfitting problem. Often high Bias implies underfitting and high Variance implies overfitting.\n",
    "- The job of the model-er is often to find that \"sweet spot\" in between!\n",
    "\n",
    "Techniques that center around reducing the Variance of a model or attempting to reduce the model from overfitting to the training are types of **Regularization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194c9baf-4377-4b83-baa8-7b9ec3e4dc37",
   "metadata": {},
   "source": [
    "## Ridge and Lasso Regression\n",
    "- These two regularization methods concern the *size* of the coefficients in linear or logistic regression.\n",
    "- Note the size of the coefficients in the polynomial regression above!\n",
    "\n",
    "Idea behind reducing the size of the cofficients:\n",
    "- Makes it harder for the model to overfit, less able to be \"squiggly\"\n",
    "- Reduces the effect of multicolinearity, often one variable will \"take over\" the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f13f26-b3fa-42c4-bb45-d55e07d7a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the taxi data\n",
    "df_taxis = sns.load_dataset('taxis')\n",
    "df_taxis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a382603-5656-491d-8e6e-faf79f19e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fun I'm going to make a new variable\n",
    "df_taxis['distance_ft'] = df_taxis['distance'] * 5280\n",
    "df_taxis[['distance', 'distance_ft']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d5591-1a03-45b6-aa3c-3630803a007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict distance from total fare\n",
    "model = ols(formula = 'distance_ft ~ total + passengers', data=df_taxis)\n",
    "res = model.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdecd018-bfab-4543-a89b-8a6000992d38",
   "metadata": {},
   "source": [
    "- Why are the values so large?\n",
    "- Why might it be a bad idea to limit the size of the coefficients here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16833385-92f1-4cb6-b36b-cc19c43cb7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxis[['distance_ft', 'passengers', 'total']].boxplot(figsize=(15,7)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c95f242-177e-47e0-8070-d9e276697ed8",
   "metadata": {},
   "source": [
    "Often **Normalizing** the data prevents issues such as this. It also may drastically improve your model as variables at different scales can make numerical optimization techniques less effective.\n",
    "\n",
    "To normalize a variable $X$ we create a new variable\n",
    "\n",
    "$$\n",
    "X' = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where $\\mu, \\sigma$ are the mean and standard deviation of $X$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90f427a-f3c3-48b7-9ce2-10d9e0120ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_taxis.copy()[['distance', 'distance_ft', 'passengers', 'total']]\n",
    "df_taxis_normalized = (df - df.mean()) / df.std()\n",
    "df_taxis_normalized.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a1dc9-2b4e-450d-91a6-378fd220ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxis_normalized.boxplot(figsize=(15,7)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e04ef9-d2d9-4755-8810-2ee9c959416d",
   "metadata": {},
   "source": [
    "- Notice what happens to distance vs. distance_ft when normalized!\n",
    "- Note that the $R^2$ below did not change, but the coefficients did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52647c3c-fcef-4d23-a716-75ac5e48645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols(formula = 'distance_ft ~ total + passengers', data=df_taxis_normalized)\n",
    "res = model.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced01da-ec56-409a-a27b-3db563606326",
   "metadata": {},
   "source": [
    "You can also do min-max normalization to put everything on a $[0,1]$ scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e5671f-6306-4b5c-9c93-e42e30c41923",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_taxis.copy()[['distance', 'distance_ft', 'passengers', 'total']]\n",
    "df_taxis_normalized = (df - df.min()) / (df.max() - df.min())\n",
    "df_taxis_normalized.boxplot(figsize=(15,7)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15931f8-2750-455a-bbac-87535309eb11",
   "metadata": {},
   "source": [
    "### WARNING\n",
    "- If we do a train-test split why is it data leakage to normalize the variables against the *entire* dataset??\n",
    "- Always remember to use the same normalization method/parameters at prediction time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab845935-fe89-45e2-9086-2a740f891dce",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "- Idea: Add a penalty term to the loss function. For example when there are $N$ datapoints and $n$ independent variables.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= \\left(\\sum_{i=1}^N (y_i - \\hat{y}_i)^2\\right) + \\lambda \\left(\\sum_{i=0}^n \\beta_0^2\\right)\\\\\n",
    "&= RSS + \\lambda ||\\beta||_2^2\n",
    "\\end{align}\n",
    "$$\n",
    "- Here $\\lambda$ controls the magnitude of the penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb342b-3cfa-47ce-a060-a4f447260306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pen = sns.load_dataset('penguins').dropna().reset_index(drop=True)\n",
    "df_pen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef857fa-1e75-4dbd-a958-6a017c055884",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(df_pen, test_size=0.95, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6dd60b-2fa5-400c-bef9-4a5fcfd3bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the numerical columns\n",
    "for col in ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']:\n",
    "    \n",
    "    train_mean = train[col].mean()\n",
    "    train_std = train[col].std()\n",
    "    \n",
    "    train[col] = ((train[col] - train_mean) / train_std)\n",
    "    val[col] = ((val[col] - train_mean) / train_std)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ab9411-a978-483a-a2a3-4e81c98e7219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Aside: see what happens if I give the model a categorical variable\n",
    "model = ols(formula = 'bill_depth_mm ~ bill_length_mm + flipper_length_mm + body_mass_g', data=train)\n",
    "res = model.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658bb856-7420-44b6-b2f5-ac71dfda0fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = res.predict(val)\n",
    "\n",
    "# score the model\n",
    "y = val['bill_depth_mm']\n",
    "y_mean = train['bill_depth_mm'].mean()  # use the mean of the training set\n",
    "\n",
    "TSS = sum((y - y_mean)**2)\n",
    "RSS = sum((y - y_pred)**2)\n",
    "print(f'This model has an R^2 on the validation set of {(TSS - RSS) / TSS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d16b4a-e12d-409c-a2a1-30c62eea6d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very small training set\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb07ff-5bfa-4a07-b42f-c9d8f7686db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember there is some multicolinearity as well\n",
    "df_pen.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1553272-105d-4246-8b42-3d5403c79770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn has a nice ridge regression model\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "x_train = train[['bill_length_mm', 'flipper_length_mm', 'body_mass_g']]\n",
    "y_train = train['bill_depth_mm']\n",
    "\n",
    "model = Ridge(alpha=10.0)\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "y_pred = model.predict(x_train)\n",
    "\n",
    "# score the model\n",
    "r2 = model.score(x_train, y_train)\n",
    "print(f'This model has an R^2 on the train set of {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b0008-14a9-4802-bbdc-7dc7e840d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = val[['bill_length_mm', 'flipper_length_mm', 'body_mass_g']]\n",
    "y_val = val['bill_depth_mm']\n",
    "\n",
    "r2 = model.score(x_val, y_val)\n",
    "print(f'This model has an R^2 on the validation set of {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3497280c-dd95-4371-bf9f-3fcc500c594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also known as hyperparamter tuning\n",
    "for alpha in [0.01, 0.1, 0.5, 1, 2, 5, 10, 20, 50, 100]:\n",
    "    \n",
    "    # create and evaluate the model for different values of alpha\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(x_train,y_train)\n",
    "    r2 = model.score(x_val, y_val)\n",
    "    \n",
    "    print(f'This model has an R^2 on the validation set of {r2} when alpha is {alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61be63a-bc18-4e46-88b8-695e1ee00f51",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "\n",
    "- Idea: Add a different penalty term to the loss function. For example when there are $N$ datapoints and $n$ independent variables.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= \\left(\\sum_{i=1}^N (y_i - \\hat{y}_i)^2\\right) + \\lambda \\left(\\sum_{i=0}^n |\\beta_0|\\right)\\\\\n",
    "&= RSS + \\lambda ||\\beta||_1\n",
    "\\end{align}\n",
    "$$\n",
    "- Here $\\lambda$ controls the magnitude of the penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d3b238-d3b5-4539-9a73-563cc3ab94ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn has a nice ridge regression model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "x_train = train[['bill_length_mm', 'flipper_length_mm', 'body_mass_g']]\n",
    "y_train = train['bill_depth_mm']\n",
    "\n",
    "model = Lasso(alpha=0.01)\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "y_pred = model.predict(x_train)\n",
    "\n",
    "# score the model\n",
    "r2 = model.score(x_train, y_train)\n",
    "print(f'This model has an R^2 on the train set of {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605e54a-0823-4db3-8c3b-5b700a2fb2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = val[['bill_length_mm', 'flipper_length_mm', 'body_mass_g']]\n",
    "y_val = val['bill_depth_mm']\n",
    "\n",
    "r2 = model.score(x_val, y_val)\n",
    "print(f'This model has an R^2 on the validation set of {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cefb096-0ec0-473c-9825-e08e541ce01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in [0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1]:\n",
    "    \n",
    "    # create and evaluate the model for different values of alpha\n",
    "    model = Lasso(alpha=alpha)\n",
    "    model.fit(x_train,y_train)\n",
    "    r2 = model.score(x_val, y_val)\n",
    "    \n",
    "    print(f'This model has an R^2 on the validation set of {r2} when alpha is {alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e5f55-67a9-4f2a-a215-a1df30129b75",
   "metadata": {},
   "source": [
    "### Ridge vs. Lasso\n",
    "\n",
    "- Both apply a penalty based on the magnitude of the coefficients\n",
    "- Ridge tends to lower the magnitude of all the coefficients somewhat equally\n",
    "- Lasso tends to \"zero out\" some of the coefficients (often is used for feature selection)\n",
    "- Check out this article [here](https://explained.ai/regularization/index.html) by Terence Parr, former Professor at USF who is now at Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ccaeb-584f-4e37-a7b2-3c711f0fd3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over many different alpha and see the coefficients\n",
    "rows = []\n",
    "for alpha in [0.001, 0.001, 0.01, 0.1, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]:\n",
    "    \n",
    "    # create and evaluate the model for different values of alpha\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(x_train,y_train)\n",
    "    \n",
    "    row = {'alpha': alpha,\n",
    "           'intercept': model.intercept_}\n",
    "    for i, coef in enumerate(model.coef_):\n",
    "        row[f'beta_{i}'] = coef\n",
    "    rows.append(row)\n",
    "    \n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e42a9-0e44-4d11-8ea9-c79121d7a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over many different alpha and see the coefficients\n",
    "rows = []\n",
    "for alpha in [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1]:\n",
    "    \n",
    "    # create and evaluate the model for different values of alpha\n",
    "    model = Lasso(alpha=alpha)\n",
    "    model.fit(x_train,y_train)\n",
    "    \n",
    "    row = {'alpha': alpha,\n",
    "           'intercept': model.intercept_}\n",
    "    for i, coef in enumerate(model.coef_):\n",
    "        row[f'beta_{i}'] = coef\n",
    "    rows.append(row)\n",
    "    \n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430150c7-970d-478e-8d53-e3d036b71a6b",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "- How do we find the best alpha? What if our choice of validation set biased which alpha was the best?\n",
    "- How do we get a less biased estimation of a metric?\n",
    "- Idea: Split the data into disjoint pieces, then train on the complement of the piece and score on the piece.\n",
    "- $k$-fold CV: Split data into $k$ equal pieces. Create and score $k$ models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99f9ccd-9e05-4235-916b-5feca2b13b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do 9-fold cross-validation with basic Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# shuffle the data\n",
    "df = df_pen.sample(frac=1, random_state=2020)\n",
    "\n",
    "rows = []\n",
    "for i in range(9):\n",
    "    \n",
    "    # 9 equal splits \n",
    "    val = df[i*(37):(i+1)*37]\n",
    "    train = df[~df.index.isin(val.index)]\n",
    "    \n",
    "    x_train = train[['bill_length_mm', 'flipper_length_mm', 'body_mass_g']]\n",
    "    y_train = train['bill_depth_mm']\n",
    "    \n",
    "    x_val = val[['bill_length_mm', 'flipper_length_mm', 'body_mass_g']]\n",
    "    y_val = val['bill_depth_mm']\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    r2 = model.score(x_val, y_val)\n",
    "    print(r2)\n",
    "    row = {'R^2' : model.score(x_val, y_val)}\n",
    "    rows.append(row)\n",
    "    \n",
    "df = pd.DataFrame(rows)\n",
    "print('The Average R^2 is', df['R^2'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffeb843-73e9-420f-9ef3-d534e4bf6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = Ridge()\n",
    "\n",
    "# can pass other parameters as well here!\n",
    "params = {'alpha' : [0.001, 0.001, 0.01, 0.1, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]}\n",
    "\n",
    "# define the search\n",
    "search = GridSearchCV(model, params, scoring='r2', cv=9)\n",
    "\n",
    "# shuffle the data, GridSearch does not shuffle\n",
    "df = df_pen.sample(frac=1, random_state=2020)\n",
    "x = df[['bill_length_mm', 'flipper_length_mm', 'body_mass_g']]\n",
    "y = df['bill_depth_mm']\n",
    "\n",
    "# execute search\n",
    "result = search.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e25398-b0ae-4411-805f-e149997bd8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.best_score_, result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf1b51a-9903-4917-bd7c-600d7d17d2d8",
   "metadata": {},
   "source": [
    "- How many models did the above grid search create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090cd12-65e4-4526-ac6b-a1a71b669108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
